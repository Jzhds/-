{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at <ipython-input-1-0ab1766d4534>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-0ab1766d4534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at <ipython-input-1-0ab1766d4534>:4 "
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://head:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1499cabc5f98>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataframe处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 用spark进行数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规定导入数据的模式，相当于dataframe中提前规定好各个列的类型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "peopleSchema = StructType([\n",
    "    # Define the name field  \n",
    "    StructField('name', StringType(), True),\n",
    "    # Add the age field  \n",
    "    StructField('age', IntegerType(), True),\n",
    "    # Add the city field  \n",
    "    StructField('city', StringType(), True)  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV file containing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.format('csv').load(name='/spark_data/rawdata.csv', schema=peopleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice1.1: Defining a schema:\n",
    "\n",
    "Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "\n",
    "    Name\n",
    "    Age\n",
    "    City\n",
    "\n",
    "The Name and City columns are StringType() and the Age column is an IntegerType()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "people_schema = StructType([\n",
    "    StructField('name',StringType(),False),\n",
    "    StructField('age',IntegerType(),False),\n",
    "    StructField('city',StringType(),False)\n",
    "])\n",
    "peopledf1 = spark.read.format('csv').load(name='/spark_data/rawdata.csv',schema = people_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopledf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 不可变性和懒惰计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不可变性的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a new data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Jennifer S. Gates', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Philip T. Kingston', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Mayor', VOTER_NAME='Michael S. Rawlings', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Adam Medrano', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Casey Thomas', year=None)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = spark.read.csv('spark_data/DallasCouncilVoters.csv',header=True) # 这种读取csv的方法比上面的要简单，更符合python的特点\n",
    "voter_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Making changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Jennifer S. Gates', year='2017', fullyear=4017.0),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Philip T. Kingston', year='2017', fullyear=4017.0),\n",
       " Row(DATE='02/08/2017', TITLE='Mayor', VOTER_NAME='Michael S. Rawlings', year='2017', fullyear=4017.0),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Adam Medrano', year='2017', fullyear=4017.0),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Casey Thomas', year=None, fullyear=None)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('fullyear',     \n",
    "                               voter_df.year + 2000) # 添加新列\n",
    "voter_df.head(5)\n",
    "#voter_df = voter_df.drop(voter_df.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44637"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('fullyear',voter_df.year + 2000)\n",
    "voter_df = voter_df.drop(voter_df.year)\n",
    "voter_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice1.2: Using lazy processing\n",
    "For this exercise, we'll be defining a Data Frame (aa_dfw_df) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2018|         0005|                          498|    hnl|\n",
      "|       01/01/2018|         0007|                          501|    ogg|\n",
      "|       01/01/2018|         0043|                            0|    dtw|\n",
      "|       01/01/2018|         0051|                          100|    stl|\n",
      "|       01/01/2018|         0075|                          147|    dca|\n",
      "|       01/01/2018|         0096|                           92|    stl|\n",
      "|       01/01/2018|         0103|                          227|    sjc|\n",
      "|       01/01/2018|         0119|                          517|    ogg|\n",
      "|       01/01/2018|         0123|                          489|    hnl|\n",
      "|       01/01/2018|         0128|                          141|    mco|\n",
      "|       01/01/2018|         0132|                          201|    ewr|\n",
      "|       01/01/2018|         0140|                          215|    sjc|\n",
      "|       01/01/2018|         0174|                          140|    rdu|\n",
      "|       01/01/2018|         0190|                           68|    sat|\n",
      "|       01/01/2018|         0200|                          215|    sfo|\n",
      "|       01/01/2018|         0209|                          169|    mia|\n",
      "|       01/01/2018|         0217|                          178|    las|\n",
      "|       01/01/2018|         0229|                          534|    koa|\n",
      "|       01/01/2018|         0244|                          115|    cvg|\n",
      "|       01/01/2018|         0262|                          159|    mia|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.csv('./spark_data/AA_DFW_2018_Departures_Short.csv.gz',header = True)\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 理解 parquet：就是储存的一种形式，用spark处理这种数据的话速度可能更快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with parquet files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reading Parquet files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " # 读取的两种方式\n",
    "df = spark.read.format('parquet').load('filename.parquet')\n",
    "df = spark.read.parquet('filename.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 写的两种方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.write.parquet('filename.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('spark_data/DallasCouncilVoters.csv',header=True)\n",
    "df.write.parquet(\"df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Jennifer S. Gates', year='2017')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet and SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_df = spark.read.parquet('df.parquet')\n",
    "flight_df.createOrReplaceTempView('flights')\n",
    "short_flights_df = spark.sql('SELECT * FROM flights WHERE year = 2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Jennifer S. Gates', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Philip T. Kingston', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Mayor', VOTER_NAME='Michael S. Rawlings', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Adam Medrano', year='2017'),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Casey Thomas\"', year='2017')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_flights_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice1.31: Saving a DataFrame in Parquet format\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it.\n",
    "\n",
    "The spark object and the df1 and df2 DataFrames have been setup for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139358\n",
      "df2 Count: 140604\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "df1 = spark.read.csv('spark_data/AA_DFW_2017_Departures_Short.csv',header=True)\n",
    "df2 = spark.read.csv('spark_data/AA_DFW_2016_Departures_Short.csv',header=True)\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date (MM/DD/YYYY)='01/01/2017', Flight Number='0005', Destination Airport='HNL', Actual elapsed time (Minutes)='537')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark 中的dataframe的列名重命名\n",
    "df3=df3.withColumnRenamed(df3.columns[0],'Date').withColumnRenamed(df3.columns[1],'Flight_number').withColumnRenamed(df3.columns[2],'Destination_port').withColumnRenamed(df3.columns[3],'elapsed_time_minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, Flight_number: string, Destination_port: string, elapsed_time_minutes: string]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279962\n"
     ]
    }
   ],
   "source": [
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice1.32: SQL and Parquet\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file).\n",
    "\n",
    "The spark object and the AA_DFW_ALL.parquet file are available for you automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 151 minutes\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('./AA_DFW_ALL.parquet',header=True)\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(elapsed_time_minutes) from flights').collect()[0] # 用collect获取到sql中提取到的信息\n",
    "print('The average flight time is: %d minutes' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分：Spark中dataframe的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 DataFrame 列操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe refresher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rows where name starts with \"M\" \n",
    "voter_df.filter(voter_df.VOTER_NAME.like('M%'))\n",
    "# Return name and position only\n",
    "voters = voter_df.select('VOTER_NAME', 'DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#练习使用sql\n",
    "voter_df.createOrReplaceTempView('voters')\n",
    "voters_num = spark.sql(\"select count(*) from voters where Date = '02/08/2017'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(voters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common dataframe transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Filter / Where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, fullyear: double]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.filter(voter_df.DATE > '1/1/2018') # or voter_df.where(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, fullyear: double]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.filter(voter_df.VOTER_NAME.like(\"A%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VOTER_NAME: string]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.select(voter_df.VOTER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VOTER_NAME: string]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#或者用另一种方法\n",
    "voter_df.select(\"VOTER_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, fullyear: double, year: int]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.withColumn('year', F.year(F.date_format(voter_df.DATE,'dd/MM/yyyy'))) #添加新列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, fullyear: double]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.drop('unused_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, full_year: double]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = voter_df.withColumnRenamed('fullyear',\"full_year\")\n",
    "voter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, fullyear: double]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.filter(voter_df['VOTER_NAME'].isNotNull())\n",
    "voter_df.filter(F.year(F.date_format(voter_df.DATE,'dd/MM/yyyy')) > 1800)\n",
    "voter_df.where(voter_df['TITLE'].contains('mayor'))\n",
    "voter_df.where(~ voter_df.TITLE.isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column string transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contained in pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applied per column as transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, full_year: double, upper: string]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.withColumn('upper', F.upper('VOTER_NAME')) #voter_df本身没有发生变化，只是新生成了个新对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, full_year: double]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can create intermediary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Jennifer S. Gates', full_year=4017.0, splits=['Jennifer', 'S.', 'Gates']),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Philip T. Kingston', full_year=4017.0, splits=['Philip', 'T.', 'Kingston']),\n",
       " Row(DATE='02/08/2017', TITLE='Mayor', VOTER_NAME='Michael S. Rawlings', full_year=4017.0, splits=['Michael', 'S.', 'Rawlings']),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Adam Medrano', full_year=4017.0, splits=['Adam', 'Medrano']),\n",
       " Row(DATE='02/08/2017', TITLE='Councilmember', VOTER_NAME='Casey Thomas', full_year=None, splits=['Casey', 'Thomas'])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df1 = voter_df.withColumn('splits', F.split('VOTER_NAME',' '))\n",
    "voter_df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can cast to other types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, full_year: double, year: int]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df.withColumn('year', voter_df['full_year'].cast(IntegerType())) #改变数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.11: Filtering column content with Python\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame voter_df contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District              |\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Michael S. Rawlings                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and  the  methods  of  assessing  special  assessments  on  Dallas  hotels  with    100 or more rooms                                                                                                                           |\n",
      "|Kevin Felder                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Adam Medrano                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Casey  Thomas                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|null                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll  (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District |\n",
      "|2018                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|011018__42                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Mark  Clayton                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Casey Thomas                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Sandy  Greyson                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Casey  Thomas\"                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Mark Clayton                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Jennifer S.  Gates                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Tiffinni A. Young                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Casey Thomas\"                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|  the  final  2018 Assessment  Plan   and   the   2018 Assessment   Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing   classifications  for the apportionment of costs and the  methods  of  assessing  special  assessments for the services and improvements  to  property  in  the  District;  closing the hearing and  levying  a  special  assessment  on  property  in  the  District       |\n",
      "|B. Adam  McGough                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|Omar Narvaez                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Philip T. Kingston                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Rickey D. Callahan                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Dwaine R. Caraway                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Philip T.  Kingston                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Jennifer S. Gates                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Lee M. Kleinman                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Monica R. Alonzo                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll   (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District|\n",
      "|Rickey D.  Callahan                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Carolyn King Arnold                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District               |\n",
      "|Erik Wilson                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|  the  final  2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District; closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District                 |\n",
      "|Lee Kleinman                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select(voter_df['VOTER_NAME']).distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.12: Modifying DataFrame columns\n",
    "Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - first_name and last_name. She asks you to split the VOTER_NAME column into words on any space character. You'll treat the last word as the last_name, and all other words as the first_name. You'll be using some new functions in this exercise including .split(), .size(), and .getItem(). The .getItem(index) takes an integer value to return the appropriately numbered item in the column. The functions .split() and .size() are in the pyspark.sql.functions library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+---------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|full_year|first_name|last_name|\n",
      "+----------+-------------+-------------------+---------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|   4017.0|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|   4017.0|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   4017.0|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|   4017.0|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     null|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|     null|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     null|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|     null|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     null|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     null|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|     null|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|     null|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|     null|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     null|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|      Casey Thomas\"|   4017.0|     Casey|  Thomas\"|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   4017.0|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|   4017.0|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|   4017.0|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|   4018.0|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|   4018.0|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+---------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0)) # 提取对应列表中的第一个值\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show() # show作为展示的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conditional DataFrame column operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/u2020100177/spark_data/example_data2.2.csv;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-6a2e76daa305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# .when(<if condition>, <then x>)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./spark_data/example_data2.2.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAge\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Adult\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/u2020100177/spark_data/example_data2.2.csv;"
     ]
    }
   ],
   "source": [
    "# .when(<if condition>, <then x>)\n",
    "df = spark.read.csv('./spark_data/example_data2.2.csv',header=True)\n",
    "df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, CASE WHEN (Age >= 18) THEN Adult WHEN (Age < 18) THEN Minor END: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple .when()\n",
    "df.select(df.Name, df.Age, \n",
    "        F.when(df.Age >= 18, \"Adult\")\n",
    "        .when(df.Age < 18, \"Minor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, CASE WHEN (Age >= 18) THEN Adult ELSE Minor END: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .otherwise() is like else\n",
    "df.select(df.Name, df.Age,\n",
    "        F.when(df.Age >= 18, \"Adult\")\n",
    "        .otherwise(\"Minor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.21: when() example\n",
    "The when() clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our voter_df DataFrame to add a random number to any voting member that is defined as a \"Councilmember\".\n",
    "\n",
    "The voter_df DataFrame is defined and available to you. The pyspark.sql.functions library is available as F. You can use F.rand() to generate the random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------+----------+---------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|fullyear|first_name|last_name|          random_val|\n",
      "+----------+-------------+-------------------+--------+----------+---------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  4017.0|  Jennifer|    Gates| 0.20874034947022668|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|  4017.0|    Philip| Kingston|  0.9799904274449358|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|  4017.0|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|  4017.0|      Adam|  Medrano|  0.4259884690119301|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|    null|     Casey|   Thomas|  0.2852335617708246|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|    null|   Carolyn|   Arnold|  0.3143523605472355|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|    null|     Scott|   Griggs|  0.7511832745410593|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|    null|        B.|  McGough|  0.4753859324898415|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|    null|       Lee| Kleinman|  0.8477099173657826|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    null|     Sandy|  Greyson|0.058053649359448345|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|    null|  Jennifer|    Gates|  0.5490611523377954|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    null|    Philip| Kingston|0.059858972694580403|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|    null|   Michael| Rawlings|                null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|    null|      Adam|  Medrano|  0.5124730281312408|\n",
      "|02/08/2017|Councilmember|      Casey Thomas\"|  4017.0|     Casey|  Thomas\"| 0.12191726204579778|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|  4017.0|   Carolyn|   Arnold|  0.7557490698109046|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|  4017.0|    Rickey| Callahan| 0.41205320414640345|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  4017.0|  Jennifer|    Gates|  0.7020024297858422|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|  4018.0|     Sandy|  Greyson|  0.7288811589636355|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  4018.0|  Jennifer|    Gates| 0.03099155272742149|\n",
      "+----------+-------------+-------------------+--------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.22: When / Otherwise\n",
    "This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your voter_df DataFrame to add a random number to any voting member that is defined as a Councilmember. Use 2 for the Mayor and 0 for anything other position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------+----------+---------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|fullyear|first_name|last_name|         random_val|\n",
      "+----------+-------------+-------------------+--------+----------+---------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  4017.0|  Jennifer|    Gates| 0.5331693319303494|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|  4017.0|    Philip| Kingston|  0.742954133135259|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|  4017.0|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|  4017.0|      Adam|  Medrano|0.42359456000443174|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|    null|     Casey|   Thomas| 0.7781923096370825|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|    null|   Carolyn|   Arnold| 0.5128597088090691|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|    null|     Scott|   Griggs|  0.799667122371296|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|    null|        B.|  McGough|  0.384386870424427|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|    null|       Lee| Kleinman|  0.804105548631092|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|    null|     Sandy|  Greyson| 0.9050508095094472|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|    null|  Jennifer|    Gates|0.12774873398500963|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|    null|    Philip| Kingston|  0.986793095567775|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|    null|   Michael| Rawlings|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|    null|      Adam|  Medrano| 0.5339576382250864|\n",
      "|02/08/2017|Councilmember|      Casey Thomas\"|  4017.0|     Casey|  Thomas\"|0.05609978352520362|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|  4017.0|   Carolyn|   Arnold| 0.4346994487968322|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|  4017.0|    Rickey| Callahan|0.09446751330458725|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|  4017.0|  Jennifer|    Gates|0.38535656598021295|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|  4018.0|     Sandy|  Greyson| 0.7986989633675667|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|  4018.0|  Jennifer|    Gates| 0.9473951148519008|\n",
      "+----------+-------------+-------------------+--------+----------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+-----------------+--------+----------+---------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|fullyear|first_name|last_name|random_val|\n",
      "+----------+--------------------+-----------------+--------+----------+---------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|  4018.0|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    null|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|    null|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|  4018.0|    Dwaine|  Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|  4018.0|      Adam|  Medrano|       0.0|\n",
      "|06/20/2018|       Mayor Pro Tem|Dwaine R. Caraway|    null|    Dwaine|  Caraway|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|    null|      Adam|  Medrano|       0.0|\n",
      "|08/15/2018|Deputy Mayor Pro Tem|     Adam Medrano|  4018.0|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|Deputy Mayor Pro Tem|     Adam Medrano|    null|      Adam|  Medrano|       0.0|\n",
      "|09/18/2018|       Mayor Pro Tem|   Casey  Thomas\"|  4018.0|     Casey|  Thomas\"|       0.0|\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|  4018.0|      Adam|  Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|    null|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|  4018.0|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|  4018.0|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|    null|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|    null|      Adam|  Medrano|       0.0|\n",
      "|04/11/2018|       Mayor Pro Tem|Dwaine R. Caraway|  4018.0|    Dwaine|  Caraway|       0.0|\n",
      "|06/13/2018|Deputy Mayor Pro Tem|     Adam Medrano|  4018.0|      Adam|  Medrano|       0.0|\n",
      "|06/13/2018|       Mayor Pro Tem|Dwaine R. Caraway|    null|    Dwaine|  Caraway|       0.0|\n",
      "|04/11/2018|Deputy Mayor Pro Tem|     Adam Medrano|    null|      Adam|  Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+--------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 User defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse string UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dene a Python method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverseString(mystr):\n",
    "    return mystr[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wrap the function and store as a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfReverseString = F.udf(reverseString, returnType = StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----+\n",
      "|      DATE|        TITLE|         VOTER_NAME|year|\n",
      "+----------+-------------+-------------------+----+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|2017|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|2017|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|2017|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|2017|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|null|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|null|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|null|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|null|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|null|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|null|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|null|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|null|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|null|\n",
      "|02/08/2017|Councilmember|      Casey Thomas\"|2017|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|2017|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|2017|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|2017|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|2018|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|2018|\n",
      "+----------+-------------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df = spark.read.csv('./spark_data/DallasCouncilVoters.csv',header=True)\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-1cf797676aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m user_df = user_df.withColumn('ReverseName',\n\u001b[0;32m----> 2\u001b[0;31m                 udfReverseString(user_df.Name)) #udf 在使用上面有些问题\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1401\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Name'"
     ]
    }
   ],
   "source": [
    "user_df = user_df.withColumn('ReverseName',\n",
    "                udfReverseString(user_df.Name)) #udf 在使用上面有些问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|year|First_name|Last_name|\n",
      "+----------+-------------+-------------------+----+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|2017|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|2017|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|2017|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|2017|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|null|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|null|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|null|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|null|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|null|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|null|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|null|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|null|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|null|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|null|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|      Casey Thomas\"|2017|     Casey|  Thomas\"|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|2017|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|2017|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|2017|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|2018|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|2018|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#练习一下F函数中的split函数，提取First name和last name\n",
    "user_df = user_df.withColumn('splits',F.split('VOTER_NAME',\" \"))\n",
    "user_df = user_df.withColumn(\"First_name\",user_df.splits.getItem(0))\n",
    "user_df = user_df.withColumn(\"Last_name\",user_df.splits.getItem(F.size(user_df.splits)-1))\n",
    "user_df = user_df.drop(\"splits\")\n",
    "# user_df = user_df.withColumn(\"Reversed_name\",udfReverseString(user_df.First_name))\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'reverseString(First_name)[2]'>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(udfReverseString(user_df['First_name'])).getItem(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argument-less example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortingCap():\n",
    "    return random.choice(['G','H','R','S'])\n",
    "udfSortingCap = F.udf(sortingCap, StringType())\n",
    "user_df = user_df.withColumn('Class', udfSortingCap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.3: Using user defined functions in Spark\n",
    "For this exercise, we'll use our voter_df DataFrame, but you're going to replace the first_name column with the first and middle names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+---------+----------+---------+--------------------+---------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|full_year|first_name|last_name|              splits|first_and_middle_name|\n",
      "+----------+-------------+-------------------+---------+----------+---------+--------------------+---------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|   4017.0|  Jennifer|    Gates|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|   4017.0|    Philip| Kingston|[Philip, T., King...|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|   4017.0|   Michael| Rawlings|[Michael, S., Raw...|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|   4017.0|      Adam|  Medrano|     [Adam, Medrano]|                 Adam|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     null|     Casey|   Thomas|     [Casey, Thomas]|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|     null|   Carolyn|   Arnold|[Carolyn, King, A...|         Carolyn King|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|     null|     Scott|   Griggs|     [Scott, Griggs]|                Scott|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|     null|        B.|  McGough| [B., Adam, McGough]|              B. Adam|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|     null|       Lee| Kleinman|     [Lee, Kleinman]|                  Lee|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|     null|     Sandy|  Greyson|    [Sandy, Greyson]|                Sandy|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|     null|  Jennifer|    Gates|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|     null|    Philip| Kingston|[Philip, T., King...|            Philip T.|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|     null|   Michael| Rawlings|[Michael, S., Raw...|           Michael S.|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     null|      Adam|  Medrano|     [Adam, Medrano]|                 Adam|\n",
      "|02/08/2017|Councilmember|      Casey Thomas\"|   4017.0|     Casey|  Thomas\"|    [Casey, Thomas\"]|                Casey|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   4017.0|   Carolyn|   Arnold|[Carolyn, King, A...|         Carolyn King|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|   4017.0|    Rickey| Callahan|[Rickey, D., Call...|            Rickey D.|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|   4017.0|  Jennifer|    Gates|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|   4018.0|     Sandy|  Greyson|    [Sandy, Greyson]|                Sandy|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|   4018.0|  Jennifer|    Gates|[Jennifer, S., Ga...|          Jennifer S.|\n",
      "+----------+-------------+-------------------+---------+----------+---------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('splits', F.split(voter_df['VOTER_NAME'], '\\s+'))\n",
    "\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Partitioning and lazy processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.41: Adding an ID Field\n",
    "\n",
    "When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset.\n",
    "\n",
    "The spark session and a Spark DataFrame df containing the DallasCouncilVotes.csv.gz file are available in your workspace. The pyspark.sql.functions library is available under the alias F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.bash_logout',\n",
       " '.ssh',\n",
       " '.profile',\n",
       " '.viminfo',\n",
       " 'test_for_glh.ipynb',\n",
       " '.config',\n",
       " '.gtkrc-2.0',\n",
       " '.mozilla',\n",
       " '.bash_history',\n",
       " 'AA_DFW_ALL.parquet',\n",
       " '.cache',\n",
       " '.local',\n",
       " '.dbus',\n",
       " 'df.parquet',\n",
       " '.bash_profile',\n",
       " '.Xauthority',\n",
       " '.scala_history',\n",
       " '.ipython',\n",
       " 'spark_data',\n",
       " 'DataCleaning_Section1-4(1).ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " '.bashrc',\n",
       " 'AA_DFW_2018_Departures_Short.csv.gz',\n",
       " 'nohup.out']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()\n",
    "# ?os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 39 rows in the voter_df DataFrame.\n",
      "\n",
      "+--------------------+-------------+\n",
      "|          VOTER_NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = spark.read.csv('spark_data/DallasCouncilVoters.csv',header=True)\n",
    "voter_df = voter_df.select(voter_df['VOTER_NAME']).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.42: IDs with different partitions\n",
    "\n",
    "You've just completed adding an ID field to a DataFrame. Now, take a look at what happens when you do the same thing on DataFrames containing a different number of partitions.\n",
    "\n",
    "To check the number of partitions, use the method .rdd.getNumPartitions() on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 200 partitions in the voter_df DataFrame.\n",
      "\n",
      "\n",
      "There are 200 partitions in the voter_df_single DataFrame.\n",
      "\n",
      "+--------------------+-------------+\n",
      "|          VOTER_NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-------------+\n",
      "|          VOTER_NAME|       ROW_ID|\n",
      "+--------------------+-------------+\n",
      "|        Lee Kleinman|1709396983808|\n",
      "|  the  final  201...|1700807049217|\n",
      "|         Erik Wilson|1700807049216|\n",
      "|  the  final   20...|1683627180032|\n",
      "| Carolyn King Arnold|1632087572480|\n",
      "| Rickey D.  Callahan|1597727834112|\n",
      "|   the   final  2...|1443109011456|\n",
      "|    Monica R. Alonzo|1382979469312|\n",
      "|     Lee M. Kleinman|1228360646656|\n",
      "|   Jennifer S. Gates|1194000908288|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df_single = voter_df.select('VOTER_NAME')\n",
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
    "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice2.43: More ID tricks\n",
    "\n",
    "Once you define a Spark process, you'll likely want to use it many times. Depending on your needs, you may want to start your IDs at a certain value so there isn't overlap with previous runs of the Spark task. This behavior is similar to how IDs would behave in a relational database. You have been given the task to make sure that the IDs output from a monthly Spark task start at the highest value from the previous month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df = spark.read.csv('./spark_data/DallasCouncilVotes.csv',header=True)\n",
    "voter_df_march = voter_df.select(voter_df.VOTER_NAME,F.when(voter_df.DATE=='03/01/2018',''))\n",
    "voter_df_march = voter_df_march.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df_april = voter_df.select(voter_df.VOTER_NAME,F.when(voter_df.DATE=='04/11/2018',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "| 44624|\n",
      "| 44625|\n",
      "| 44626|\n",
      "| 44627|\n",
      "| 44628|\n",
      "| 44629|\n",
      "| 44630|\n",
      "| 44631|\n",
      "| 44632|\n",
      "| 44633|\n",
      "| 44634|\n",
      "| 44635|\n",
      "| 44636|\n",
      "| 44637|\n",
      "| 44638|\n",
      "| 44639|\n",
      "| 44640|\n",
      "| 44641|\n",
      "| 44642|\n",
      "| 44643|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频3.1 缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在行动前调用.cache()\n",
    "voter_df = spark.read.csv('voter_data.txt.gz',header=True)\n",
    "voter_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+----+\n",
      "|      DATE|        TITLE|         VOTER_NAME|  ID|\n",
      "+----------+-------------+-------------------+----+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|null|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|null|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|null|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|null|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|null|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|null|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|null|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|null|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|null|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|null|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|null|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|null|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|null|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|null|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|null|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|null|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates|null|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson|null|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates|null|\n",
      "+----------+-------------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('ID',voter_df.DATE+1)\n",
    "voter_df = voter_df.cache()\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#通过is_cached检查缓存状态\n",
    "print(voter_df.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DATE: string, TITLE: string, VOTER_NAME: string, ID: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#调用unpersist()释放缓存\n",
    "voter_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.1 Caching a DataFrame 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139359 rows took 3.661939 seconds\n",
      "Counting 139359 rows again took 0.881640 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "departures_df=spark.read.csv( \"AA_DFW_2017.csv.gz\")\n",
    "#csv:AA_DFW_2017.csv.gz\n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.2 Removing a DataFrame from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "#csv:AA_DFW_2017.csv.gz\n",
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频3.2 How to split objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use OS utilities/scripts(split,cut,awk)\n",
    "#split -l 10000 -d largefile chunk-\n",
    "#Use custom scripts Write out to Parquet\n",
    "df_csv = spark.read.csv('AA_DFW_2014_Departures_Short.csv')\n",
    "df_csv.write.parquet('data.parquet',mode='overwrite')\n",
    "df = spark.read.parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.3 File import performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File import performance\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "You have two types of files available: departures_full.txt.gz and departures_xxx.txt.gz where xxx is 000 - 013. The same number of rows is split between each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t139359\n",
      "Time to run: 0.163822\n"
     ]
    }
   ],
   "source": [
    "\"\"\"departures_000.txt.gz  departures_005.txt.gz  departures_010.txt.gz\n",
    "departures_001.txt.gz  departures_006.txt.gz  departures_011.txt.gz\n",
    "departures_002.txt.gz  departures_007.txt.gz  departures_012.txt.gz\n",
    "departures_003.txt.gz  departures_008.txt.gz  departures_013.txt.gz\n",
    "departures_004.txt.gz  departures_009.txt.gz  departures_full.txt.gz\n",
    "\"\"\"\n",
    "# File import performance\n",
    "#You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "#You have two types of files available: departures_full.txt.gz and departures_xxx.txt.gz where xxx is 000 - 013. The same number of rows is split between each file.\n",
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "#split_df = spark.read.csv('departures_0*.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "#start_time_b = time.time()\n",
    "#print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "#print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.4 Reading Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 41135\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.5 Writing Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 200\n",
      "Partition count after change: 500\n"
     ]
    }
   ],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频3.4 performance improvements"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Inline calculations\n",
    "df = df.read.csv('datafile')\n",
    "df = df.withColumn('avg', (df.total_sales / df.sales_count))\n",
    "df = df.withColumn('sq_ft', df.width * df.length)\n",
    "df = df.withColumn('total_avg_size', udfComputeTotal(df.entries) / df.numEntries)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Explaining the Spark execution plan\n",
    "voter_df = df.select(df['NAME']).distinct()\n",
    "voter_df.explain()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Broadcasting\n",
    "from pyspark.sql.functions import broadcast\n",
    "combined_df = df.join(broadcast(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.7 Normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [Destination Airport#587], [IATA#610], Inner, BuildRight\n",
      ":- *(2) Project [Date (MM/DD/YYYY)#585, Flight Number#586, Destination Airport#587, Actual elapsed time (Minutes)#588]\n",
      ":  +- *(2) Filter isnotnull(Destination Airport#587)\n",
      ":     +- FileScan csv [Date (MM/DD/YYYY)#585,Flight Number#586,Destination Airport#587,Actual elapsed time (Minutes)#588] Batched: false, DataFilters: [isnotnull(Destination Airport#587)], Format: CSV, Location: InMemoryFileIndex[file:/home/u2020100177/spark_data/AA_DFW_2018_Departures_Short.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true])), [id=#677]\n",
      "   +- *(1) Project [AIRPORTNAME#609, IATA#610]\n",
      "      +- *(1) Filter isnotnull(IATA#610)\n",
      "         +- FileScan csv [AIRPORTNAME#609,IATA#610] Batched: false, DataFilters: [isnotnull(IATA#610)], Format: CSV, Location: InMemoryFileIndex[file:/home/u2020100177/spark_data/airportnames.txt.gz], PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<AIRPORTNAME:string,IATA:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "flights_df=spark.read.csv('spark_data/AA_DFW_2018_Departures_Short.csv.gz',header=True)\n",
    "airports_df=spark.read.csv('spark_data/airportnames.txt.gz',header=True)\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.8 Using broadcasting on Spark joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [Destination Airport#587], [IATA#610], Inner, BuildRight\n",
      ":- *(2) Project [Date (MM/DD/YYYY)#585, Flight Number#586, Destination Airport#587, Actual elapsed time (Minutes)#588]\n",
      ":  +- *(2) Filter isnotnull(Destination Airport#587)\n",
      ":     +- FileScan csv [Date (MM/DD/YYYY)#585,Flight Number#586,Destination Airport#587,Actual elapsed time (Minutes)#588] Batched: false, DataFilters: [isnotnull(Destination Airport#587)], Format: CSV, Location: InMemoryFileIndex[file:/home/u2020100177/spark_data/AA_DFW_2018_Departures_Short.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true])), [id=#709]\n",
      "   +- *(1) Project [AIRPORTNAME#609, IATA#610]\n",
      "      +- *(1) Filter isnotnull(IATA#610)\n",
      "         +- FileScan csv [AIRPORTNAME#609,IATA#610] Batched: false, DataFilters: [isnotnull(IATA#610)], Format: CSV, Location: InMemoryFileIndex[file:/home/u2020100177/spark_data/airportnames.txt.gz], PartitionFilters: [], PushedFilters: [IsNotNull(IATA)], ReadSchema: struct<AIRPORTNAME:string,IATA:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习3.9 Comparing broadcast vs normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t119910\tduration: 21.268620\n",
      "Broadcast count:\t119910\tduration: 16.830747\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用广播功能可以提高查询效率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频4.1 Introduction to data pipelines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "schema = StructType([StructField('name', StringType(), False),StructField('age', StringType(), False)])\n",
    "df = spark.read.format('csv').load('datafile').schema(schema)\n",
    "df = df.withColumn('id', monotonically_increasing_id())\n",
    "...\n",
    "df.write.parquet('outdata.parquet')\n",
    "df.write.json('outdata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.1 Quick pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('2015-departures.csv.gz', header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df[3] > 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json('output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频4.2 Data handling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('AA_DFW_2017.csv.gz', comment='#')\n",
    "df1 = spark.read.csv('AA_DFW_2017.csv.gz', header='True')\n",
    "df1 = spark.read.csv('AA_DFW_2017.csv.gz', sep=',')\n",
    "df1 = spark.read.csv('AA_DFW_2017.csv.gz', sep='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.2 Removing commented lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full count: 32794\n",
      "Comment count: 1416\n",
      "Remaining count: 31378\n"
     ]
    }
   ],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "\n",
    "def col(col_name):\n",
    "    return annotations_df[col_name]\n",
    "\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.3 Removing invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count: 31378\n",
      "Final count: 20580\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "initial_count=31378\n",
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df[\"colcount\"] < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.4 Splitting into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df[\"_c0\"], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.5 Further parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType,StringType\n",
    "def retriever(cols, colcount):\n",
    "  # Return a list of dog data\n",
    "  return cols[4:colcount]\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Create a new column using your UDF\n",
    "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "\n",
    "# Remove the original column, split_cols, and the colcount\n",
    "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频4.3 Data validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parsed_df = spark.read.parquet('parsed_data.parquet')\n",
    "company_df = spark.read.parquet('companies.parquet')\n",
    "verified_df = parsed_df.join(company_df, parsed_df.company == company_df.company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.6 Validate rows via join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7de668f27c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Count the number of rows in split_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msplit_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Join the DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "valid_folders_df= spark.read.csv(\"spark_data/valid_folders.txt\")\n",
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.7 Examining invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " split_df:\t32794\n",
      " joined_df:\t20945\n",
      " invalid_df: \t11849\n",
      "11223 distinct invalid folders found\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 视频4.4 Final analysis and delivery"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Analysis calculations(UDF)\n",
    "#Calculations using UDF\n",
    "def getAvgSale(saleslist):\n",
    "    totalsales = 0  \n",
    "    count = 0\n",
    "    for sale in saleslist:    \n",
    "        totalsales += sale[2] + sale[3]    \n",
    "        count += 2\n",
    "    return totalsales / count\n",
    "udfGetAvgSale = F.udf(getAvgSale, DoubleType())\n",
    "df = df.withColumn('avg_sale', udfGetAvgSale(df.sales_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Inline calculations\n",
    "df = df.read.csv('datafile')\n",
    "df = df.withColumn('avg', (df.total_sales / df.sales_count))\n",
    "df = df.withColumn('sq_ft', df.width * df.length)\n",
    "df = df.withColumn('total_avg_size', udfComputeTotal(df.entries) / df.numEntries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.8 Dog parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|dog_list                          |\n",
      "+----------------------------------+\n",
      "|[affenpinscher,0,9,173,298]       |\n",
      "|[Border_terrier,73,127,341,335]   |\n",
      "|[kuvasz,0,0,499,327]              |\n",
      "|[Great_Pyrenees,124,225,403,374]  |\n",
      "|[schipperke,146,29,416,309]       |\n",
      "|[groenendael,168,0,469,374]       |\n",
      "|[Bedlington_terrier,10,12,462,332]|\n",
      "|[Lhasa,39,1,499,373]              |\n",
      "|[Kerry_blue_terrier,17,16,300,482]|\n",
      "|[vizsla,112,93,276,236]           |\n",
      "+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType,StringType,StructType,StructField,IntegerType\n",
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.9 Per image count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|size(dogs)|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "  dogs = []\n",
    "  for dog in doglist:\n",
    "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "  return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习4.10 Percentage dog pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
      "|  folder|       filename|width|height|                dogs|dog_pixels|      dog_percent|\n",
      "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
      "|02110627|n02110627_12938|  200|   300|[[affenpinscher, ...|     49997|83.32833333333333|\n",
      "|02104029|   n02104029_63|  500|   375|[[kuvasz, 0, 0, 4...|    163173|          87.0256|\n",
      "|02105056| n02105056_2834|  500|   375|[[groenendael, 16...|    112574|60.03946666666666|\n",
      "|02093647|  n02093647_541|  500|   333|[[Bedlington_terr...|    144640|86.87087087087087|\n",
      "|02098413| n02098413_1355|  500|   375|[[Lhasa, 39, 1, 4...|    171120|           91.264|\n",
      "|02093859| n02093859_2309|  330|   500|[[Kerry_blue_terr...|    131878|79.92606060606062|\n",
      "|02109961| n02109961_1017|  475|   500|[[Eskimo_dog, 43,...|    189189|79.65852631578947|\n",
      "|02108000| n02108000_3491|  600|   450|[[EntleBucher, 30...|    168667|62.46925925925926|\n",
      "|02085782| n02085782_1731|  600|   449|[[Japanese_spanie...|    250125|92.84521158129176|\n",
      "|02110185| n02110185_2736|  259|   500|[[Siberian_husky,...|    113088|87.32664092664093|\n",
      "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "  totalpixels = 0\n",
    "  for dog in doglist:\n",
    "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "  return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.where('dog_percent > 60').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get() missing 1 required positional argument: 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-910a7a872875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get() missing 1 required positional argument: 'key'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
